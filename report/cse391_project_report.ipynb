{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jonathan Mathai 110320715 \n",
    "## Richu Jacob 110257792\n",
    "## Omar Syed 110484590\n",
    "\n",
    "#                                                <center>Youtube Trending Data</center>\n",
    "\n",
    "\n",
    "### Problem: Predicting whether a video will be trending or not \n",
    "### Libraries used: pandas, numpy, tensorflow, urllib, json, tabulate, re\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Data gathering </center>\n",
    "### <ol> <li> Scrape the internet for list of trending and nontrending videos using Selenium webdriver. </li><li>For each video extract the video title and video id and add to dataframe</li>Used the YouTube API to add views, likes, dislikes, comments, category id, and tags length features to each data<li>Compiled around 6500 trending videos and 15900 nontrending videos</li> </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneVideoStats(video_id, api_key):\n",
    "    searchUrl=\"https://www.googleapis.com/youtube/v3/videos?id=\"+video_id+\"&key=\"+api_key+\"&part=statistics,snippet,content_details\"\n",
    "    response = urllib.request.urlopen(searchUrl).read()\n",
    "    data = json.loads(response)\n",
    "    try:\n",
    "        viewCount = data['items'][0]['statistics']['viewCount']\n",
    "        likeCount = data['items'][0]['statistics']['likeCount']\n",
    "        dislikeCount = data['items'][0]['statistics']['dislikeCount']\n",
    "        commentCount = data['items'][0]['statistics']['commentCount']\n",
    "        categoryId = data['items'][0]['snippet']['categoryId']\n",
    "        tagLength = len(data['items'][0]['snippet']['tags'])\n",
    "        likeRatioCount = int(likeCount)/(int(likeCount)+int(dislikeCount))\n",
    "        commentRatioCount = int(commentCount)/int(viewCount)\n",
    "        return [[viewCount,likeCount,dislikeCount,commentCount, categoryId, tagLength]]\n",
    "    except (KeyError, IndexError):\n",
    "        return\n",
    "    \n",
    "def standardize(a):\n",
    "    d = np.array(a)\n",
    "    d = d.astype('float32')\n",
    "    d -= np.mean(d, axis=0)\n",
    "    d /= np.std(d, axis=0)\n",
    "    return np.array(d)\n",
    "\n",
    "def getApiKey(filename):\n",
    "    api_key_file = open(filename, 'r')\n",
    "    return api_key_file.read().rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Preparing data </center> \n",
    "### <ol><li>Created a new dataframe out of 6000 trending and 6000 nontrending data</li><li>Standardized the dataframe </li> </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Preprocessing Neural Net </center>\n",
    "## Classifier Architecture\n",
    "![dff](deep_feed_forward.jpg)\n",
    "### <ol> <li>Created input, hidden, and output layers sequentially using keras</li><li>Dropped any duplicates from dataframe </li><li>Passed the converted dataframe into deep feed forward model</li><li>We used a deep feed forward neural network. Composed of 1 input layer and 2 hidden layers and 1 output.</li></ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Optimizing Neural Net </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "all_data=[]\n",
    "for i in range(5):\n",
    "    nonTrendingDataSet = getNonTrendingSample(nonTrendingTrainSet)\n",
    "    setTuple = getFullTrainingSet(subsetTrending, nonTrendingDataSet)\n",
    "    fullSubset = setTuple[0]\n",
    "    all_data = setTuple[0]\n",
    "    trendingLabels = setTuple[1]\n",
    "    scaleMatrixData = standardizeData(fullSubset)\n",
    "    model.fit(scaleMatrixData, trendingLabels, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ol> <li>Our input layer and hidden layers contain 512 nodes and our output layer is 2 nodes, because our results are trending and nontrending</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = keras.Sequential([\n",
    "keras.layers.Dense(512, input_dim=6,activation=tf.nn.relu),\n",
    "keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "(keras.layers.Dropout(0.50)),\n",
    "keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "(keras.layers.Dropout(0.50)),\n",
    "keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ol><li>Testing accuracy was 50 prior to standardizing data </li>\n",
    "## <li>Used standard scalar input data to reduce overfitting of the data.</li>\n",
    "## <li> Used drop out layer to reduce overfitting after each hidden layer </li> </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    trendingSet = pd.read_csv('trending_dataset.csv')\n",
    "    nontrendingSet = pd.read_csv('nontrending_dataset.csv')\n",
    "    trendingSet = trendingSet[['views', 'likes', 'dislikes', 'comments', 'category_id', 'tags_length']]\n",
    "    nontrendingSet = nontrendingSet[['views', 'likes', 'dislikes', 'comments', 'category_id', 'tags_length']]\n",
    "    #trendingSet['likes_ratio'] = trendingSet['likes']/(trendingSet['likes']+trendingSet['dislikes'])\n",
    "    #trendingSet['comments_ratio'] = trendingSet['comments']/(trendingSet['views'])\n",
    "    \n",
    "    #nontrendingSet['likes_ratio'] = nontrendingSet['likes']/(nontrendingSet['likes']+nontrendingSet['dislikes'])\n",
    "    #nontrendingSet['comments_ratio'] = nontrendingSet['comments']/(nontrendingSet['views'])\n",
    "    \n",
    "    \n",
    "    #print(trendingSet)\n",
    "    trending_stats_non = trendingSet.values\n",
    "    nontrending_stats_non = nontrendingSet.values\n",
    "    #print(trending_stats_non.shape)\n",
    "    #print(nontrending_stats_non)\n",
    "    \n",
    "    #nontrending_stats_non = np.load('nontrending_stats.npy')\n",
    "    #trending_stats_non = np.load('trending_stats.npy')\n",
    "    # combine trending and nontrending data, then standardize them\n",
    "    all_data = np.concatenate((trending_stats_non[0:6000,:], nontrending_stats_non[0:6000]), axis=0)\n",
    "    \n",
    "    #all_data = all_data.astype('float32')\n",
    "    #print(all_data.shape)\n",
    "    standardized_data = standardize(all_data)\n",
    "    # sample tuple for a random video (views, likes, dislikes, commentCount)\n",
    "    exampleTuple = [[1713501,84894,2855,15155, 20, 12]]\n",
    "    print(all_data.shape)\n",
    "    exampleTuple = standardizeTuple(exampleTuple, all_data)\n",
    "    # split em up\n",
    "    trending_stats = standardized_data[0:6000,:]\n",
    "    nontrending_stats = standardized_data[6001:,:]\n",
    "    # curate sets of data necessary for neural net\n",
    "    train_data = np.concatenate((trending_stats[0:2900,:], nontrending_stats[0:2900,:]), axis = 0)\n",
    "    test_data = np.concatenate((trending_stats[2901:5801,:], nontrending_stats[2901:5801,:]), axis = 0)\n",
    "    train_labels = np.concatenate((np.ones(2900), np.zeros(2900)), axis = 0)\n",
    "    test_labels = np.concatenate((np.ones(2900), np.zeros(2900)), axis = 0)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "                    loss ='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    #train_data = train_data.sample(frac=1)\n",
    "    model.fit(train_data, train_labels, epochs=100)\n",
    "    predictions = model.predict(test_data)\n",
    "    #print(predictions)\n",
    "    #cm = tf.contrib.metrics.confusion_matrix(test_labels, predictions)\n",
    "    #print(cm)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "\n",
    "    print(\"\\n\\nTest Set Accuracy:\" + str(test_acc))\n",
    "\n",
    "    #history = model.fit(X, Y, validation_split=0.33, epochs=1000, batch_size=10, verbose=0, callbacks=[tb, early_stop])\n",
    "    # extra code so you can test individual video ids and see if trending or nontrending\n",
    "    apiKey = getApiKey('api_key.txt')\n",
    "    videoId = ''\n",
    "    while videoId != 'quit':\n",
    "        print(\"Predict a video by entering a video id (or type 'quit' to exit): \")\n",
    "        videoId = input()\n",
    "        video = np.array(getOneVideoStats(videoId, apiKey), dtype='|S10').astype(float)\n",
    "        # print table of video data\n",
    "        print(\"\\n\" + tabulate(video, headers=['ViewCount', 'LikeCount', 'DislikeCount', 'CommentCount', 'CategoryId', 'Tags']))\n",
    "        video = standardizeTuple(video, all_data)\n",
    "        # if video data was empty dont do a prediction\n",
    "        if (len(video) != 0):\n",
    "            prediction = model.predict(video)\n",
    "            # print table of prediction probabilities\n",
    "            print(\"\\n\" + tabulate(prediction, headers=['P(Non-Trending)', 'P(Trending)']))\n",
    "            # print prediction for video\n",
    "            if (np.argmax(prediction) == 1):\n",
    "                print(\"\\nPredicted Trending Video\\n\")\n",
    "            else:\n",
    "                print(\"\\nPredicted Non-Trending Video\\n\")\n",
    "        else:\n",
    "            print(\"Video missing views, likes, dislikes, or commentCount...\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> End product </center>\n",
    "### <ol> <li> User enters video id to get prognosis whether it is trending </li></ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36b_env",
   "language": "python",
   "name": "py_36b_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
